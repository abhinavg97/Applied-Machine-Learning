{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        import numpy \n",
    "        self.np=numpy\n",
    "        import copy\n",
    "        self.copy=copy\n",
    "        self.overall_data=[]\n",
    "        \n",
    "    def preprocess(self):\n",
    "        with open(self.tf,\"r\") as f:\n",
    "            for line in f:\n",
    "                data=line.split(\", \")\n",
    "                data[0]=int(data[0])\n",
    "                data[2]=int(data[2])\n",
    "                data[4]=int(data[4])\n",
    "                data[10]=int(data[10])\n",
    "                data[11]=int(data[11])\n",
    "                data[12]=int(data[12])\n",
    "                data[14]=int(data[14])\n",
    "                self.overall_data.append(data)\n",
    "\n",
    "       \n",
    "    # method to calculate the next best feature of a node via entropy calculation\n",
    "    # this method returns a bunch of values \n",
    "    \n",
    "    #def entropy(self,examplestoconsider,label):\n",
    "    def entropy(self,label):\n",
    "        \n",
    "        #lst contains the samples to be considered for selecting best feature\n",
    "        lst=[]\n",
    "        \n",
    "        # ls contains samples which contains feature[i]\n",
    "        ls=[[] for i in range(14)]\n",
    "        \n",
    "        #dont_consider those features which are already present in the tree above this node\n",
    "        dont_consider=[]\n",
    "        for i in range(14):\n",
    "            if((label[i])!=[]):\n",
    "                dont_consider.append(i)\n",
    "                \n",
    "                \n",
    "        # populating the ls list , considering examples to consider\n",
    "        for i in range(len(self.overall_data)):\n",
    "            lst.append(i)\n",
    "            if label[0]!=[] and self.overall_data[i][0]==label[0]:\n",
    "                ls[0].append(i)\n",
    "            if label[1]!=[] and self.overall_data[i][1]==label[1]:\n",
    "                ls[1].append(i)\n",
    "            if label[2]!=[] and self.overall_data[i][2]==label[2] :\n",
    "                ls[2].append(i)\n",
    "            if label[3]!=[] and self.overall_data[i][3]==label[3]:\n",
    "                ls[3].append(i)\n",
    "            if label[4]!=[] and self.overall_data[i][4]==label[4]:\n",
    "                ls[4].append(i)\n",
    "            if label[5]!=[] and self.overall_data[i][5]==label[5]:\n",
    "                ls[5].append(i)\n",
    "            if label[6]!=[] and self.overall_data[i][6]==label[6]:\n",
    "                ls[6].append(i)\n",
    "            if label[7]!=[] and self.overall_data[i][7]==label[7]:\n",
    "                ls[7].append(i)\n",
    "            if label[8]!=[] and self.overall_data[i][8]==label[8]:\n",
    "                ls[8].append(i)\n",
    "            if label[9]!=[] and self.overall_data[i][9]==label[9]:\n",
    "                ls[9].append(i)\n",
    "            if label[10]!=[] and self.overall_data[i][10]==label[10]:\n",
    "                ls[10].append(i)\n",
    "            if label[11]!=[] and self.overall_data[i][11]==label[11]:\n",
    "                ls[11].append(i)\n",
    "            if label[12]!=[] and self.overall_data[i][12]==label[12]:\n",
    "                ls[12].append(i)\n",
    "            if label[13]!=[] and self.overall_data[i][13]==label[13]:\n",
    "                ls[13].append(i)\n",
    "        \n",
    "        # taking the intersection of the ls list values to get the lst \n",
    "        for i in range(14):\n",
    "            if len(ls[i]):\n",
    "                lst = list(set(ls[i]) & set(lst))\n",
    "                  \n",
    "        # dictionary to store class information of a feature \n",
    "        dict=[{} for i in range(15)]\n",
    "        age_d={}\n",
    "        fnl_d={}\n",
    "        ednum_d={}\n",
    "        cg_d={}\n",
    "        cl_d={}\n",
    "        hpw_d={}\n",
    "        for examples_number in lst:\n",
    "            data = self.overall_data[examples_number]\n",
    "            for i in range(15):\n",
    "                if i!=0 and i!=2 and i!=4 and i!=10 and i!=11 and i!=12:\n",
    "                    if data[i] in dict[i]:\n",
    "                        if data[14] == 0:\n",
    "                            dict[i][data[i]] = self.np.add( dict[i][data[i]] ,[1,0])\n",
    "                        else:\n",
    "                            dict[i][data[i]] = self.np.add( dict[i][data[i]] ,[0,1])\n",
    "                    else:\n",
    "                        if data[14] == 0:\n",
    "                            dict[i][data[i]] = self.np.array([1,0])\n",
    "                        else:\n",
    "                            dict[i][data[i]] = self.np.array([0,1])\n",
    "                elif i==0:\n",
    "                    if data[i] in age_d:\n",
    "                        if data[14]==0:\n",
    "                            age_d[data[i]] = self.np.add( age_d[data[i]] ,[1,0])\n",
    "                        else:\n",
    "                            age_d[data[i]] = self.np.add( age_d[data[i]] ,[0,1])\n",
    "                    else:\n",
    "                        if data[14]==0:\n",
    "                            age_d[data[i]] = self.np.array([1,0])\n",
    "                        else:\n",
    "                            age_d[data[i]] = self.np.array([0,1])\n",
    "\n",
    "                elif i==2:\n",
    "                    if data[i] in fnl_d:\n",
    "                        if data[14]==0:\n",
    "                            fnl_d[data[i]] = self.np.add( fnl_d[data[i]] ,[1,0])\n",
    "                        else:\n",
    "                            fnl_d[data[i]] = self.np.add( fnl_d[data[i]] ,[0,1])\n",
    "                    else:\n",
    "                        if data[14]==0:\n",
    "                            fnl_d[data[i]] = self.np.array([1,0])\n",
    "                        else:\n",
    "                            fnl_d[data[i]] = self.np.array([0,1])\n",
    "\n",
    "                elif i==4:\n",
    "                    if data[i] in ednum_d:\n",
    "                        if data[14]==0:\n",
    "                            ednum_d[data[i]] = self.np.add( ednum_d[data[i]] ,[1,0])\n",
    "                        else:\n",
    "                            ednum_d[data[i]] = self.np.add( ednum_d[data[i]] ,[0,1])\n",
    "                    else:\n",
    "                        if data[14]==0:\n",
    "                            ednum_d[data[i]] = self.np.array([1,0])\n",
    "                        else:\n",
    "                            ednum_d[data[i]] = self.np.array([0,1])\n",
    "\n",
    "                elif i==10:\n",
    "                    if data[i] in cg_d:\n",
    "                        if data[14]==0:\n",
    "                            cg_d[data[i]] = self.np.add( cg_d[data[i]] ,[1,0])\n",
    "                        else:\n",
    "                            cg_d[data[i]] = self.np.add( cg_d[data[i]] ,[0,1])\n",
    "                    else:\n",
    "                        if data[14]==0:\n",
    "                            cg_d[data[i]] = self.np.array([1,0])\n",
    "                        else:\n",
    "                            cg_d[data[i]] = self.np.array([0,1])\n",
    "\n",
    "                elif i==11:\n",
    "                    if data[i] in cl_d:\n",
    "                        if data[14]==0:\n",
    "                            cl_d[data[i]] = self.np.add( cl_d[data[i]] ,[1,0])\n",
    "                        else:\n",
    "                            cl_d[data[i]] = self.np.add( cl_d[data[i]] ,[0,1])\n",
    "                    else:\n",
    "                        if data[14]==0:\n",
    "                            cl_d[data[i]] = self.np.array([1,0])\n",
    "                        else:\n",
    "                            cl_d[data[i]] = self.np.array([0,1])\n",
    "\n",
    "                else:\n",
    "                    if data[i] in hpw_d:\n",
    "                        if data[14]==0:\n",
    "                            hpw_d[data[i]] = self.np.add( hpw_d[data[i]] ,[1,0])\n",
    "                        else:\n",
    "                            hpw_d[data[i]] = self.np.add( hpw_d[data[i]] ,[0,1])\n",
    "                    else:\n",
    "                        if data[14]==0:\n",
    "                            hpw_d[data[i]] = self.np.array([1,0])\n",
    "                        else:\n",
    "                            hpw_d[data[i]] = self.np.array([0,1])\n",
    "        \n",
    "        \n",
    "        # calculating bins for the continous data , I have constructed 10 bins for the coninous features\n",
    "\n",
    "        age_list=sorted(age_d.items(), key=lambda s: s[0])\n",
    "        fnl_list=sorted(fnl_d.items(), key=lambda s: s[0])\n",
    "        ednum_list=sorted(ednum_d.items(), key=lambda s: s[0])\n",
    "        cg_list=sorted(cg_d.items(), key=lambda s: s[0])\n",
    "        cl_list=sorted(cl_d.items(), key=lambda s: s[0])\n",
    "        hpw_list=sorted(hpw_d.items(), key=lambda s: s[0])\n",
    "\n",
    "\n",
    "\n",
    "        #0\n",
    "        i=0\n",
    "\n",
    "        if(len(age_list)):\n",
    "            min_age = age_list[0][0]\n",
    "            max_age = age_list[-1][0]\n",
    "\n",
    "            range_age = max_age - min_age\n",
    "\n",
    "            for key in age_list:\n",
    "                age = key[0]\n",
    "                if range_age == 0:\n",
    "                    bin = 1\n",
    "                else:\n",
    "                    bin = (int(10*(age-min_age)/range_age))+1\n",
    "                if age==max_age:\n",
    "                    bin=bin-1\n",
    "                bin_value = ((bin-1)*range_age/10)+min_age\n",
    "                if bin_value in dict[i]:\n",
    "                    dict[i][bin_value] = self.np.add(dict[i][bin_value],key[1])\n",
    "                else :\n",
    "                    dict[i][bin_value] = key[1]\n",
    "\n",
    "\n",
    "        #2\n",
    "        i=2\n",
    "        if(len(fnl_list)):\n",
    "            min_fnl = fnl_list[0][0]\n",
    "            max_fnl = fnl_list[-1][0]\n",
    "\n",
    "            range_fnl = max_fnl - min_fnl\n",
    "            for key in fnl_list:\n",
    "                fnl = key[0]\n",
    "                if range_fnl == 0:\n",
    "                    bin = 1\n",
    "                else:\n",
    "                    bin = (int(10*(fnl-min_fnl)/range_fnl))+1\n",
    "                if fnl==max_fnl:\n",
    "                    bin=bin-1\n",
    "                bin_value = ((bin-1)*range_fnl/10)+min_fnl\n",
    "                if bin_value in dict[i]:\n",
    "                    dict[i][bin_value] = self.np.add(dict[i][bin_value],key[1])\n",
    "                else :\n",
    "                    dict[i][bin_value] = key[1]\n",
    "\n",
    "\n",
    "\n",
    "        #4\n",
    "        i=4\n",
    "        if(len(ednum_list)):\n",
    "            min_ednum = ednum_list[0][0]\n",
    "            max_ednum = ednum_list[-1][0]\n",
    "\n",
    "            range_ednum = max_ednum - min_ednum\n",
    "            for key in ednum_list:\n",
    "                ednum = key[0]\n",
    "                if range_ednum == 0:\n",
    "                    bin = 1\n",
    "                else:\n",
    "                    bin = (int(10*(ednum-min_ednum)/range_ednum))+1\n",
    "                if ednum==max_ednum:\n",
    "                    bin=bin-1\n",
    "                bin_value = ((bin-1)*range_ednum/10)+min_ednum\n",
    "                if bin_value in dict[i]:\n",
    "                    dict[i][bin_value] = self.np.add(dict[i][bin_value],key[1])\n",
    "                else :\n",
    "                    dict[i][bin_value] = key[1]\n",
    "\n",
    "\n",
    "        #10\n",
    "        i=10\n",
    "        if(len(cg_list)):\n",
    "            min_cg = cg_list[0][0]\n",
    "            max_cg = cg_list[-1][0]\n",
    "\n",
    "            range_cg = max_cg - min_cg\n",
    "            for key in cg_list:\n",
    "                cg = key[0]\n",
    "                if range_cg == 0:\n",
    "                    bin = 1\n",
    "                else:\n",
    "                    bin = (int(10*(cg-min_cg)/range_cg))+1\n",
    "                if cg==max_cg:\n",
    "                    bin=bin-1\n",
    "                bin_value = ((bin-1)*range_cg/10)+min_cg\n",
    "                if bin_value in dict[i]:\n",
    "                    dict[i][bin_value] = self.np.add(dict[i][bin_value],key[1])\n",
    "                else :\n",
    "                    dict[i][bin_value] = key[1]\n",
    "\n",
    "        #11\n",
    "        i=11\n",
    "        if(len(cl_list)):\n",
    "            min_cl = cl_list[0][0]\n",
    "            max_cl = cl_list[-1][0]\n",
    "\n",
    "            range_cl = max_cl - min_cl\n",
    "            for key in cl_list:\n",
    "                cl = key[0]\n",
    "                if range_cl == 0:\n",
    "                    bin = 1\n",
    "                else:\n",
    "                    bin = (int(10*(cl-min_cl)/range_cl))+1\n",
    "                if cl==max_cl:\n",
    "                    bin=bin-1\n",
    "                bin_value = ((bin-1)*range_cl/10)+min_cl\n",
    "                if bin_value in dict[i]:\n",
    "                    dict[i][bin_value] = self.np.add(dict[i][bin_value],key[1])\n",
    "                else :\n",
    "                    dict[i][bin_value] = key[1]\n",
    "\n",
    "        #12\n",
    "        i=12\n",
    "        if(len(hpw_list)):\n",
    "            min_hpw = hpw_list[0][0]\n",
    "            max_hpw = hpw_list[-1][0]\n",
    "\n",
    "            range_hpw = max_hpw - min_hpw\n",
    "            for key in hpw_list:\n",
    "                hpw = key[0]\n",
    "                if range_hpw == 0:\n",
    "                    bin = 1\n",
    "                else:\n",
    "                    bin = (int(10*(hpw-min_hpw)/range_hpw))+1\n",
    "                if hpw==max_hpw:\n",
    "                    bin=bin-1\n",
    "                bin_value = ((bin-1)*range_hpw/10)+min_hpw\n",
    "                if bin_value in dict[i]:\n",
    "                    dict[i][bin_value] = self.np.add(dict[i][bin_value],key[1])\n",
    "                else :\n",
    "                    dict[i][bin_value] = key[1]\n",
    "\n",
    "        # calculating entropy for all the features\n",
    "        entropy_main=[{} for i in range(14)]\n",
    "\n",
    "        i=0\n",
    "        for d in dict:\n",
    "            for key in d:\n",
    "                sum = self.np.sum(d[key])\n",
    "                if d[key][0]==0 or d[key][1]==0:\n",
    "                    entropy_main[i][key] = 0\n",
    "                else:\n",
    "                    entropy_main[i][key]=(-d[key][0]*self.np.log2(((1.0)*d[key][0]/sum))-d[key][1]*self.np.log2(((1.0)*d[key][1]/sum)))\n",
    "\n",
    "            i=i+1\n",
    "            if i==14:          \n",
    "                break\n",
    "\n",
    "\n",
    "        # calculating the feature label for the best feature\n",
    "        i=0\n",
    "        min_entropy=1000000\n",
    "        min_entropy_label=-1\n",
    "        for feature_dic in entropy_main:\n",
    "            sum=0\n",
    "            for key in feature_dic:\n",
    "                sum = sum + feature_dic[key]\n",
    "            if sum<min_entropy and (i not in dont_consider) and len(feature_dic):\n",
    "                min_entropy=sum\n",
    "                min_entropy_label=i\n",
    "            i=i+1\n",
    "        \n",
    "        \n",
    "        # After selecting the feature which gives minimum entropy , we make a list of its attributes\n",
    "        i = 0\n",
    "        min_entr_feature_name_list=[]\n",
    "        for feature_dic in entropy_main:\n",
    "            if i == min_entropy_label:\n",
    "                for key in feature_dic:\n",
    "                    min_entr_feature_name_list.append(key)\n",
    "                break\n",
    "            else:\n",
    "                i=i+1\n",
    "        \n",
    "        # If the feature which gives minimum entropy value is continous, we store the minimum value and range of vale of the feature\n",
    "        # if the minimum entropy label wasn't continous we set the min and range values as 0\n",
    "        if min_entropy_label == 0:\n",
    "            min_feature_value = min_age\n",
    "            range_feature_value = range_age\n",
    "        elif min_entropy_label == 2:\n",
    "            min_feature_value = min_fnl\n",
    "            range_feature_value = range_fnl\n",
    "        elif min_entropy_label == 4:\n",
    "            min_feature_value = min_ednum\n",
    "            range_feature_value = range_ednum\n",
    "        elif min_entropy_label == 10:\n",
    "            min_feature_value = min_cg\n",
    "            range_feature_value = range_cg\n",
    "        elif min_entropy_label == 11:\n",
    "            min_feature_value = min_cl\n",
    "            range_feature_value = range_cl\n",
    "        elif min_entropy_label == 12:\n",
    "            min_feature_value = min_hpw\n",
    "            range_feature_value = range_hpw\n",
    "        else:\n",
    "            min_feature_value = 0\n",
    "            range_feature_value = 0 \n",
    "        \n",
    "        # Storing the values of number of instances which have class label 0 and class label 1\n",
    "        wrongs=0\n",
    "        rights=0\n",
    "        flag=0\n",
    "        if 0 in dict[-1]:\n",
    "            flag=1\n",
    "            wrongs = self.np.sum(dict[-1][0])\n",
    "        if 1 in dict[-1]:\n",
    "            flag=1\n",
    "            rights = self.np.sum(dict[-1][1])\n",
    "\n",
    "        return min_entropy,min_entropy_label,min_feature_value,range_feature_value,min_entr_feature_name_list,rights,wrongs\n",
    "\n",
    "    \n",
    "    # Building the tree\n",
    "    def  build_tree(self,node):\n",
    "        # Setting the threshold for tree depth randomly , with min depth 5 and max depth 14\n",
    " \n",
    "        depth_threshold = self.np.random.randint(5,14) \n",
    "        \n",
    "        if node.depth<depth_threshold and node.node_entropy>0 and node.node_entropy<10000:\n",
    "            for name in node.name_list:\n",
    "                label_list = node.label_list[:]\n",
    "                label_list[node.feature_label] = name\n",
    "                #examplestoconsider,node_entropy,feature_label,min_val,range_val,name_list,rights,wrongs = self.entropy(node.examplestoconsider,label_list)\n",
    "                node_entropy,feature_label,min_val,range_val,name_list,rights,wrongs = self.entropy(label_list)\n",
    "                depth = node.depth + 1\n",
    "                #p =  Node(examplestoconsider,label_list,feature_label,min_val,range_val,node_entropy,depth,name_list,rights,wrongs)   \n",
    "                p =  Node(label_list,feature_label,min_val,range_val,node_entropy,depth,name_list,rights,wrongs)   \n",
    "                node.add_child(p)\n",
    "            \n",
    "        # building the tree recursivley for the node's children\n",
    "        for i in range(len(node.children)):\n",
    "            self.build_tree(node.children[i])\n",
    "\n",
    "        \n",
    "\n",
    "       # method to print information of a node\n",
    "    def info(self,p):\n",
    "        print \"=------------------------------------\"\n",
    "        #print \"p.examplestoconsider = \",p.examplestoconsider\n",
    "        print \"p.depth = \", p.depth   \n",
    "        print \"p.node_entropy = \",p.node_entropy  \n",
    "        print \"p.label_list = \",p.label_list    \n",
    "        print \"p.children = \",p.children           \n",
    "        print \"p.name_list = \",p.name_list    \n",
    "        print \"p.feature_label = \",p.feature_label                  \n",
    "        print \"p.min_val = \",p.min_val    \n",
    "        print \"p.range_val = \",p.range_val    \n",
    "        print \"p.rights = \",p.rights  \n",
    "        print \"p.wrongs = \",p.wrongs    \n",
    "        print \"--------------------------------------\"\n",
    "        \n",
    "    # train method to train on a test corpus\n",
    "    def train(self,tf):\n",
    "        self.tf=tf\n",
    "        self.preprocess()\n",
    "        label_list=[[] for i in range(14)]\n",
    "        #examplestoconsider = [i for i in range(len(self.overall_data))]\n",
    "        #examplestoconsider1,node_entropy,feature_label,min_val,range_val,name_list,rights,wrongs = self.entropy(examplestoconsider,label_list)\n",
    "        node_entropy,feature_label,min_val,range_val,name_list,rights,wrongs = self.entropy(label_list)\n",
    "        depth=1\n",
    "        \n",
    "        # seeding the tree , storing the information(min_feature label etc) in the root node \n",
    "        #self.root = Node(examplestoconsider,label_list,feature_label,min_val,range_val,node_entropy,depth,name_list,rights,wrongs)\n",
    "        self.root = Node(label_list,feature_label,min_val,range_val,node_entropy,depth,name_list,rights,wrongs)\n",
    "        # calling build_tree method to recursively grow the tree\n",
    "        self.build_tree(self.root)\n",
    "       \n",
    "    # prediction method to predict the class of the test corpus\n",
    "    def predict(self,test_file):\n",
    "        test_data=[]\n",
    "        prediction=[]\n",
    "        \n",
    "        # reading from the test file[s] and storing the test corpus data\n",
    "        with open(test_file,\"r\") as f:\n",
    "            for line in f:\n",
    "                data=line.split(\", \")\n",
    "                data=line.split(\", \")\n",
    "                data=line.split(\", \")\n",
    "                data[0]=int(data[0])\n",
    "                data[2]=int(data[2])\n",
    "                data[4]=int(data[4])\n",
    "                data[10]=int(data[10])\n",
    "                data[11]=int(data[11])\n",
    "                data[12]=int(data[12])\n",
    "                test_data.append(data)\n",
    "        \n",
    "        # for each data point in test corpus , predicting the class label\n",
    "\n",
    "        for dp in test_data:\n",
    "            node = self.copy.deepcopy(self.root)\n",
    "            rights = 0\n",
    "            wrongs = 0\n",
    "            # recursively descending the decision tree until we are sure about the label or we have reached a leaf\n",
    "            while node.node_entropy>0 and len(node.children):\n",
    "                feature_label = node.feature_label\n",
    "                min_val = node.min_val\n",
    "                range_val = node.range_val\n",
    "                rights = node.rights\n",
    "                wrongs = node.wrongs\n",
    "                label_list = node.label_list\n",
    "                # If the feature to be tested is continous , we fit the continous value to a bin \n",
    "                flag=1\n",
    "                if feature_label == 0 or feature_label == 2 or feature_label == 4 or feature_label == 10 or feature_label == 11 or feature_label == 12:\n",
    "                    flag=0\n",
    "                    if range_val:\n",
    "                        bin = (int(10*(dp[feature_label]-min_val)/range_val))+1\n",
    "                    else:\n",
    "                        bin = 1\n",
    "                    if bin>10:  # if the continous value maps to a bin beyond 10 , fit it in 10th bin\n",
    "                        bin = 10\n",
    "                    elif bin<1: # if the continous value maps to a bin less than 1 , fit it to 1st bin\n",
    "                        bin = 1\n",
    "                    dp[feature_label] = ((bin-1)*range_val/10)+min_val\n",
    "\n",
    "                min_bin = 1000000000000\n",
    "                flag1=1\n",
    "                # checking the current node's children for the feature to be tested \n",
    "                for i in range(len(node.children)):\n",
    "                    if node.children[i].label_list[feature_label]==dp[feature_label]:\n",
    "                        flag1=0\n",
    "                        node = node.children[i]\n",
    "                        break\n",
    "                    if feature_label==0 or feature_label==2 or feature_label==4 or feature_label==10 or feature_label==11 or feature_label==12: \n",
    "                        # if the feature value is continous , proceeding with the node with closest bin value\n",
    "                        if abs(node.children[i].label_list[feature_label]-dp[feature_label])<min_bin:\n",
    "                            min_bin = i\n",
    "                        if i == len(node.children)-1:\n",
    "                            node = node.children[min_bin]\n",
    "                # if the feature value is not present in the tree , stopping the search and predicting the class label\n",
    "                if flag1 and flag:\n",
    "                    break\n",
    "            # if class label 1's examples are more predict 1\n",
    "            if rights>wrongs:\n",
    "                prediction.append(1)\n",
    "            #otherwise if they are equal predict randomly \n",
    "            elif rights == wrongs:\n",
    "                prediction.append(self.np.random.randint(0,2))\n",
    "            #else predict 0\n",
    "            else:\n",
    "                prediction.append(0)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class to build a single node of the tree\n",
    "class Node:\n",
    "    #def __init__(self,examplestoconsider,label_list,feature_label,min_val,range_val,node_entropy,depth,name_list,rights,wrongs):\n",
    "    def __init__(self,label_list,feature_label,min_val,range_val,node_entropy,depth,name_list,rights,wrongs):\n",
    "        #self.examplestoconsider=examplestoconsider\n",
    "        self.depth=depth\n",
    "        self.node_entropy=node_entropy\n",
    "        self.label_list = label_list\n",
    "        self.children = []\n",
    "        self.name_list = name_list\n",
    "        self.feature_label = feature_label\n",
    "        self.min_val = min_val\n",
    "        self.range_val = range_val\n",
    "        self.rights = rights\n",
    "        self.wrongs = wrongs\n",
    "    def add_child(self,obj):\n",
    "        self.children.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf='/home/abhi/Desktop/Sem 5/AML/asn1/Decision Tree/train.csv'\n",
    "model = DecisionTree()\n",
    "model.train(tf)\n",
    "\n",
    "import dill as pickle\n",
    "with open('es15btech11002.model','w') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Decisions\n",
    "\n",
    "### Splitting criterion\n",
    "I used entropy as the splitting criterion and univariate splitting for each node\n",
    "\n",
    "### binning\n",
    "I have divided the continous variables into 10 bins , \n",
    "for different nodes the bin length of the same feature may be different,\n",
    "the bin length is decided like so:\n",
    "for each node , the number of examples encountered,satisfying all the questions leading to this node are considered\n",
    "the bin length then is (max_val - min_val)/10 for that feature and for that node\n",
    "\n",
    "### Trade off of time complexity of training and prediction\n",
    "I had 2 methods , in one the time comlexity of training was high and of predicting low\n",
    "In the other the time conplexity of predicting high and training low\n",
    "#### 1st method\n",
    "Going through all the examples of the entire train file to check whether the node criterion is satisfied to get the next best feature\n",
    "This will increase the time complexity of training as we each node we go through all the 10000 examples\n",
    "and the number of nodes are exponential in depth\n",
    "#### 2nd method\n",
    "The training time complexity can be reduced drastically by storing the examples satisfying the node criterion in the node\n",
    "This will alllow its children node to consider only it's(parent node's) examples \n",
    "But here the predicting time complexity increases as we make a deep copy of the root node,for all the test instances(this takes a lot of time as it has to copy the examples list which may be huge)\n",
    "\n",
    "#### I have opted for the 1st method as the prediction time for each test data point is around 8 times for the 2nd method , the training time for the 1st method is not much for the given dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
