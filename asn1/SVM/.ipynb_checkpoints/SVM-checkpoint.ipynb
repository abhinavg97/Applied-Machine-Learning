{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have mapped each unique string of the dataset to a unique number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "train_label=[]\n",
    "tf='/home/abhi/Desktop/Sem 5/AML/asn1/SVM/data/train.csv'\n",
    "def preprocess(tf):\n",
    "    with open(tf,\"r\") as f:\n",
    "        # I have used max to keep count of the number of unique elements encountered for string type features\n",
    "        # and maximum value for number type features\n",
    "        max = [0 for i in range(14) ]\n",
    "        dict = [{} for i in range(14)]\n",
    "\n",
    "        for line in f:\n",
    "            data=line.split(\", \")\n",
    "            data[0]=int(data[0])\n",
    "            data[2]=int(data[2])\n",
    "            data[4]=int(data[4])\n",
    "            data[10]=int(data[10])\n",
    "            data[11]=int(data[11])\n",
    "            data[12]=int(data[12])\n",
    "            data[14]=int(data[14])\n",
    "            for i in range(14):\n",
    "                # string type features\n",
    "                if i !=0 and i !=2 and i !=4 and i !=10 and i !=11 and i !=12 :\n",
    "                    if data[i] in dict[i]:\n",
    "                        data[i] = dict[i][data[i]]      \n",
    "                    else:\n",
    "                        dict[i][data[i]] = max[i]     # unique string elements get mapped to integer numbers here\n",
    "                        data[i] = max[i]\n",
    "                        max[i]=max[i]+1       \n",
    "                # number type features\n",
    "                else:\n",
    "                    if data[i]>max[i]:               # storing the max value encountered for a particular feature\n",
    "                        max[i] = data[i]\n",
    "        \n",
    "            train_data.append(data[:14])\n",
    "            train_label.append(data[14])\n",
    "\n",
    "    for j in range(len(train_data)):                                  # Normalizing the values so that any feature is not unduly given more importance\n",
    "        for i in range(14):\n",
    "            if(max[i]!=0) :\n",
    "                train_data[j][i] = ((train_data[j][i]*1.0/(max[i]*1.0)))\n",
    "\n",
    "preprocess(tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Kernels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def gaussianKernel(  X1,X2=None,sigma=0.4,axis=None):\n",
    "\n",
    "    if X2 is None:\n",
    "        X2=X1\n",
    "    \n",
    "    # to calculate gram matrix we split the gaussian , and apply vector operations, that is faster\n",
    "    \n",
    "    gram_matrix = np.zeros((X1.shape[0], X2.shape[0]))\n",
    "    matrix1 = np.dot(X1, X2.T)\n",
    "    matrix2 = np.dot(X1, X1.T)\n",
    "    matrix3 = np.dot(X2, X2.T)\n",
    "    \n",
    "    matrix1=np.power(np.e,matrix1/sigma**2)\n",
    "    \n",
    "    t1 = [0 for i in range((X1.shape[0]))]\n",
    "    t2 = [0 for i in range((X2.shape[0]))]\n",
    "    \n",
    "    for i in range((X1.shape[0])):\n",
    "        t1[i]=np.power(np.e,-matrix2[i][i]/(2.0*sigma**2))\n",
    "        \n",
    "    for i in range((X2.shape[0])):\n",
    "        t2[i]=np.power(np.e,-matrix3[i][i]/(2.0*sigma**2))\n",
    "    \n",
    "    t1=np.array(t1)\n",
    "    t2=np.array(t2)\n",
    "    t3=np.array([])\n",
    "    t4=np.array([])\n",
    "    t3=[t1 for i in range(X2.shape[0]) ]\n",
    "    \n",
    "    t4=[t2 for i in range(X1.shape[0]) ]\n",
    "    t3 = np.array(t3)\n",
    "    t4=np.array(t4)\n",
    "    t3=t3.T\n",
    "    \n",
    "    gram_matrix = matrix1*t3*t4\n",
    "\n",
    "    return gram_matrix\n",
    "\n",
    "def polynomialKernel(  X1,X2=None,q=2):\n",
    "    \"\"\"(Pre)calculates Gram Matrix K\"\"\"\n",
    "    if X2 is None:\n",
    "        X2=X1\n",
    "        # vectorizing\n",
    "    gram_matrix = (np.dot(X1, X2.T)+1)**q    \n",
    "    \n",
    "    return gram_matrix\n",
    "\n",
    "\n",
    "\n",
    "def linearKernel( X1,X2=None):\n",
    "    \"\"\"(Pre)calculates Gram Matrix K\"\"\"\n",
    "    if X2 is None:\n",
    "        X2=X1\n",
    "        \n",
    "    #vectorizing\n",
    "    gram_matrix = np.dot(X1, X2.T)\n",
    "\n",
    "    return gram_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MKL class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class inspired   from Github link provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiKernelfixedrules(object):\n",
    "    def __init__(self, kernels, X=None):\n",
    "        self.kernels = kernels\n",
    "        # setting gamma values to take convex sum of kernels\n",
    "        self.gammas = [0.4,0.4,0.2]\n",
    "        self.X = X\n",
    "        self.Ks = None\n",
    "        if X is not None: # Precompute kernels\n",
    "            self.Ks = [kernel(X) for kernel in kernels]\n",
    "\n",
    "    def __call__(self, X, Y=None):\n",
    "        K = 0\n",
    "        if X is self.X and (Y is X or Y is None):\n",
    "            for gamma, Ki in zip(self.gammas, self.Ks):\n",
    "                if gamma > 0.0:\n",
    "                    K += gamma * Ki\n",
    "        else:\n",
    "            for gamma, kernel in zip(self.gammas, self.kernels):\n",
    "                if gamma > 0.0:\n",
    "                    K += gamma * kernel(X, Y)\n",
    "        return K \n",
    "\n",
    "\n",
    "\n",
    "class MultiKernelSVC():\n",
    "\n",
    "    def __init__(self, kernels):\n",
    "        self.kernels = kernels\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        kernels = self.kernels\n",
    "        multi_kernel = MultiKernelfixedrules(kernels, X)\n",
    "        svc = svm.SVC(kernel=multi_kernel)\n",
    "        svc.fit(X, y)\n",
    "        self._svc = svc\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self._svc.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 fold stratified cross validation on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score multi kernel: 0.843 \n",
      "time =  13.283836\n",
      "accuracy score gaussian kernel: 0.840 \n",
      "time =  9.516997\n",
      "accuracy score linear kernel: 0.815\n",
      "time =  1.445564\n",
      "accuracy score polynomial kernel: 0.839\n",
      "time =  2.292473\n",
      "accuracy score multi kernel: 0.852 \n",
      "time =  13.128705\n",
      "accuracy score gaussian kernel: 0.853 \n",
      "time =  9.573886\n",
      "accuracy score linear kernel: 0.821\n",
      "time =  1.392076\n",
      "accuracy score polynomial kernel: 0.854\n",
      "time =  2.261654\n",
      "accuracy score multi kernel: 0.843 \n",
      "time =  13.324738\n",
      "accuracy score gaussian kernel: 0.843 \n",
      "time =  9.72512\n",
      "accuracy score linear kernel: 0.813\n",
      "time =  1.439265\n",
      "accuracy score polynomial kernel: 0.843\n",
      "time =  2.33597\n",
      "accuracy score multi kernel: 0.857 \n",
      "time =  12.88832\n",
      "accuracy score gaussian kernel: 0.853 \n",
      "time =  9.376124\n",
      "accuracy score linear kernel: 0.822\n",
      "time =  1.39498\n",
      "accuracy score polynomial kernel: 0.852\n",
      "time =  2.256032\n",
      "accuracy score multi kernel: 0.849 \n",
      "time =  12.803532\n",
      "accuracy score gaussian kernel: 0.846 \n",
      "time =  9.314794\n",
      "accuracy score linear kernel: 0.824\n",
      "time =  1.411069\n",
      "accuracy score polynomial kernel: 0.845\n",
      "time =  2.121836\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(train_data,train_label)\n",
    "\n",
    "\n",
    "# appending linear ,gaussian , polynomial kernels\n",
    "kernels=[]\n",
    "\n",
    "kernels.append(gaussianKernel)\n",
    "kernels.append(polynomialKernel)\n",
    "kernels.append(linearKernel)\n",
    "\n",
    "# 5 fold cross validation on train set\n",
    "for train,test in skf.split(train_data, train_label):\n",
    "    \n",
    "    X_train=[]\n",
    "    X_test=[]\n",
    "    Y_train=[]\n",
    "    Y_test=[]\n",
    "    \n",
    "    for i in train:\n",
    "        X_train.append(train_data[i])\n",
    "        Y_train.append(train_label[i])\n",
    "    for i in test:\n",
    "        X_test.append(train_data[i])\n",
    "        Y_test.append(train_label[i])\n",
    "    \n",
    "    X_train=np.array([np.array(xi) for xi in X_train])\n",
    "    X_test=np.array([np.array(xi) for xi in X_test])\n",
    "\n",
    "    svc_multi = MultiKernelSVC(kernels)\n",
    "    start = time.clock()\n",
    "    svc_multi.fit(X_train, Y_train)\n",
    "    y_pred =svc_multi.predict(X_test)\n",
    "    end = time.clock()\n",
    "    print 'accuracy score multi kernel: %0.3f ' % accuracy_score(Y_test, y_pred)\n",
    "    print 'time = ',end-start\n",
    "    end = time.clock()\n",
    "    \n",
    "    \n",
    "    svc_gauss = SVC(kernel=gaussianKernel)\n",
    "    start = time.clock()\n",
    "    svc_gauss.fit(X_train ,Y_train)\n",
    "    y_pred = svc_gauss.predict(X_test)\n",
    "    end = time.clock()\n",
    "    print 'accuracy score gaussian kernel: %0.3f ' % accuracy_score(Y_test, y_pred)\n",
    "    print 'time = ',end-start\n",
    "    \n",
    "    svc_lin = SVC(kernel=linearKernel)\n",
    "    start = time.clock()\n",
    "    svc_lin.fit(X_train ,Y_train)\n",
    "    y_pred = svc_lin.predict(X_test)\n",
    "    end = time.clock()\n",
    "    print 'accuracy score linear kernel: %0.3f' % accuracy_score(Y_test, y_pred)\n",
    "    print 'time = ',end-start\n",
    "    \n",
    "    \n",
    "    svc_pol = SVC(kernel=polynomialKernel)\n",
    "    start = time.clock()\n",
    "    svc_pol.fit(X_train, Y_train)\n",
    "    y_pred = svc_pol.predict(X_test)\n",
    "    end = time.clock()\n",
    "    print 'accuracy score polynomial kernel: %0.3f' % accuracy_score(Y_test, y_pred)\n",
    "    print 'time = ',end-start\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and Training time for various kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Kernel:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average  Time    : 1.534996\n",
    "\n",
    "Average Accuracy : 81.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "\n",
    "      Average Accuracy     Average Time\n",
    "\n",
    "q=2    84.66                 2.0217264\n",
    "q=3    84.52                 5.7816452 \n",
    "q=4    84.52                 6.3598108\n",
    "q=5    84.34                 7.4633032\n",
    "q=6    83.76                 10.9385442\n",
    "q=7    82.64                 19.0469488\n",
    "\n",
    "We see that the accuracy decreases as we incease q , we choose q = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Kernel\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "      Average Accuracy     Average Time\n",
    "s=0.1  80.18                 10.38\n",
    "s=0.2  83.26                 9.86\n",
    "s=0.3  84.46                 9.72\n",
    "s=0.4  84.7                  9.64\n",
    "s=0.5  84.52                 9.48\n",
    "s=0.6  84.4                  9.52\n",
    "s=0.7  84.22                 9.44\n",
    "s=0.8  84.14                 9.34\n",
    "s=0.9  83.9                  9.46\n",
    "s=1    83.7                  9.44\n",
    "\n",
    "\n",
    "We see as sigma increases , accuracy initially increase then decrease , we select the best value of sigma as 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and Training Time for Multi kernel and various kernels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "               Average Accuracy    Average Time   \n",
    "Multi Kernel       84.88               13.08         \n",
    "Gaussian Kernel    84.7                9.5013842       \n",
    "Linear Kernel      81.9                1.4165908        \n",
    "Poly Kernel        84.66               2.253593       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "### Comparision between various kernels based on performance \n",
    "\n",
    "We see that the linear kernel takes the less amount of time compared to polynomial kernal and gaussian kernel, but have the least accuracy comparitively.\n",
    "\n",
    "The accuracy of polynomial kernel with q=2 and gaussian kernel with sigma =0.4 is almost the same but polynomial kernel  , takes about 5 times less time than gaussian kernel.\n",
    "\n",
    "The best kernel for the above dataset would be polynomial kernel as its accuracy is highest among the above kernels and takes very reasonable amount of time\n",
    "\n",
    "### Comparision between multi kernel and various kernels\n",
    "\n",
    "We observe the classifiaction accuracy of multi kernel is highest compared to the component kernels, \n",
    "the convex sum used was [0.4,0.4,0.2] for gaussian, polynomial, linear kernels respectively,\n",
    "as gaussian and polynomial kernels gave higher accuracy.\n",
    "\n",
    "The accuracy of the multi kernel is higher than the weighted average of accuracies of the component kernels\n",
    "0.4x84.7 + 0.4x84.66 + 0.2x81.9 < 84.88\n",
    "\n",
    "The training time for multi kernels is higher than the component kernels \n",
    "\n",
    "The weighted average of the training time for multi kernel is higher too.\n",
    "\n",
    "\n",
    "### Time  on Normalizing values\n",
    "I have noticed normalizing feature values has a drastic effect on training time for this dataset\n",
    "\n",
    "Eg:\n",
    "\n",
    "Without normalizing linear kernel took 13 mins to train and predict and with normalizing (with 100 value range) it took only about a minute , with suprisingly almost same accuracy results\n",
    "\n",
    "Further to strengthen the arguement that normalizing has a huge impact on training and predicting time, \n",
    "the following was observed\n",
    "\n",
    "When I normalized the value so that all the value of all the features in a datapoint remain in 100 range , the training(and predicting) time was observed to be around 1 min , and normalizing it while keeping the range as 10 , the training (and predicting) time dropped to around 3 secs , around 30 times improvement.\n",
    "The accuracy for the former case was 82% and the later was 81% (not much of a difference at all!)\n",
    "\n",
    "All the features have the value between 0 and 1 for each data point in the above implementation\n",
    "\n",
    "### Time  on vectorizing the multiplication while writing kernels\n",
    "\n",
    "If we vectorize the multiplications while writing the kernels , we get a huge improvement in time.\n",
    "\n",
    "### Time on changing  kernel function (kernel matrix)\n",
    "\n",
    "If the kernel matrix used to fit the data contains big values , then it is observed the time taken to train is large compared to if we pass kernel matrix with smaller values .\n",
    "\n",
    "This is evident from the linearKernel and polynomialKernel matrix , similar method is used to compute both of them ,but the polynomial takes 100 times more time for the train and prediction.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
